<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>A Front-End Adaptation Network for Improving Speech Recognition Performance in Packet Loss and Noisy Environments</title>
        <link rel="stylesheet" type="text/css" href="styles.css">
        <script src="jquery-3.5.js"></script>
    </head>
    <body>
<div class="container">
    <div id="text1">A Front-End Adaptation Network for Improving Speech Recognition Performance in Packet Loss and Noisy Environments</div>
    <div id="intro">
        <br>
        <p>
          Yehoshua Dissen, Shiry Yonash, Israel Cohen and Joseph Keshet
      </p>
       <p>
            [<a href="https://arxiv.org/abs/2406.18928">Paper</a>]
        </p>
       <p>
            [<a href="https://github.com/MLSpeech/WhisperDenoiser">Code</a>]
        </p>
    </div>
</div>
<div class="content-container">
    <img src="Formalwhisper_denoiser.png" style="width:20%; height:auto;">
    <br>
    <p>
      Robust automatic speech recognition (ASR) in packet loss and noisy environments remains challenging. Large pretrained transformer models have made notable strides in improving ASR performance across diverse domains. However, there is still significant room for improvement, even in moderate packet loss and noise conditions. Enhancing these models is particularly difficult because retraining is computationally prohibitive, and finetuning introduces the risk of domain shift, which can degrade performance in other languages or environments.
      We introduce a novel method that leverages a front-end adaptation network to enhance word error rate (WER) performance in packet loss and noisy scenarios. Our approach addresses the constraints of working with large pretrained ASR models while avoiding retraining or finetuning.
      We connect an adaptation network to a frozen ASR model, where the network is trained to modify corrupted input spectra using both the ASR modelâ€™s loss function and an enhancement loss. This strategy allows the system to adapt to packet loss and noise without compromising the original ASR model's performance or generalization across domains.
      The method focuses on improving WER rather than signal quality or intelligibility, targeting it for ASR applications. We conduct a comprehensive set of experiments on various noise types. Our results demonstrate that the adaptation network significantly reduces WER in all conditions while preserving the foundational performance of the pretrained ASR model.
  </p>
</div>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

<div class="content-container">
    <script src="wavesurfer.js"></script>
    <div class="content-title">Single Speaker (LJ Speech Dataset)</div>
    <br>
    <h3>Original Audio </h3>
    <audio display:inline-block; title="LJ006-0034" controls="" preload="none"><source src="samples/origin_files/LJ006-0034.wav"></audio>
    <br><br>
    <table border="0" class="inlineTable">

<div class="content-container">
  <h3>Reconstructed Audio with Packet Loss Rates</h3>
  <table border="1">
      <tr>
          <th>Model</th>
          <th>0.05</th>
          <th>0.1</th>
          <th>0.2</th>
          <th>0.3</th>
          <th>0.4</th>
          <th>0.5</th>
          <th>0.6</th>
          <th>0.7</th>
      </tr>
      <tr>
          <td>FRN</td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.05.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.1.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.2.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.3.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.4.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.5.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.6.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/FRN/LJ006-0034_0.7.wav"></audio><p>Example transcript</p></td>
      </tr>
      <tr>
          <td>tPLCNet</td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.05.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.1.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.2.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.3.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.4.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.5.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.6.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/tPLCNet/LJ006-0034_0.7.wav"></audio><p>Example transcript</p></td>
      </tr>
      <tr>
          <td>Ours GL Phase</td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.05.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.1.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.2.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.3.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.4.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.5.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.6.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.7.wav"></audio><p>Example transcript</p></td>
      </tr>
      <tr>
          <td>Ours Original Phase</td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.05.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.1.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.2.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.3.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.4.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.5.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.6.wav"></audio><p>Example transcript</p></td>
          <td><audio controls preload="none"><source src="samples/Ours_GL/LJ006-0034_0.7.wav"></audio><p>Example transcript</p></td>
      </tr>
  </table>
</div>


<div class="content-container">
  <div class="content-title">REFERENCES</div>
  <ol type="1">
  <li>Sample paged based on <a style="color:rgb(22, 38, 67)"  href="https://daps.cs.princeton.edu/projects/HiFi-GAN/index.php"> HiFi-GAN</a> page.</li><br>
  <ul>
    <li>Griffin, D., & Lim, J. (1984). "Signal estimation from modified short-time Fourier transform." <i>IEEE Transactions on Acoustics, Speech, and Signal Processing</i>, <b>32</b>(2), 236-243.</li>
    <li>Nguyen, V.-A., Nguyen, A. H. T., & Khong, A. W. H. (2023). "Improving performance of real-time full-band blind packet-loss concealment with predictive network." <i>ICASSP 2023</i>.</li>
    <li>Westhausen, N. L., & Meyer, B. T. (2022). "tPLCnet: Real-time Deep Packet Loss Concealment in the Time Domain Using a Short Temporal Context." <i>Interspeech 2022</i>.</li>
    <li>Dissen, Y., Yonash, S., Cohen, I., & Keshet, J. (2024). "Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation Network." <i>Interspeech 2024</i>.</li>
</ul>
</ol> 
</div>
</body>
</html>